configfile: '/uufs/chpc.utah.edu/common/HIPAA/u1264408/u1264408/Git/SEMIColon/data/config/snakemake_config/LeeSixconfig.yaml'
in_dir: str = config["input_directory"]
out_dir: str = config["output_directory"]
reference: str = config["reference_genome"]
reference_index: str = config["reference_genome_index"]
samples: list[str] = config["samples"]
donors: list[str] = config["donors"]
nchunks = config["nchunks"]
chunks = list(range(1,nchunks+1))
chroms = ["1","2","3","4","6","7","8","9","10","11","12","13","14","15","16","17",
"18","19","20","21","22","X","Y"]

matches = {
    'HLS': {'crypt_samples': ['HLS_1C_30_B5', 'HLS_1C_30_D5', 'HLS_1C_30_G5', 'HLS_1C_30_H5', 'HLS_2C_30_D6', 'HLS_2C_30_E6']},
    'PD28690': {'crypt_samples': ['PD28690bx_2_a5', 'PD28690bx_2_d5', 'PD28690cb_2_g3', 'PD28690cb_2_h3']},
    'PD34199': {'crypt_samples': ['PD34199a_c17', 'PD34199a_c22', 'PD34199a_c27', 'PD34199a_c28', 'PD34199a_c31', 'PD34199a_c3', 'PD34199a_c5', 'PD34199a_c7', 'PD34199a_t31', 'PD34199a_t28']},
    'PD34202': {'crypt_samples': ['PD34202a_s9']}
    }

# matches = {
#     'HLS': {'crypt_samples': ['HLS_1C_30_B5']},
#     'PD34199': {'crypt_samples': ['PD34199a_t28']},
#     }

### IMPORTS ###
import os
import re

# Wildcard constraints; should all be alphanumeric
wildcard_constraints:
    sample = "[A-Za-z0-9_]+",
    chroms = "[A-Za-z0-9]+",
    i = "[0-9]+",
    out_dir = out_dir,
    donor = "[A-Za-z0-9]+"


rule all:
    input:
        expand("{out_dir}/bam/{sample}-sorted.bam.bai", out_dir = out_dir, sample = samples, donor = donors),
        expand("{out_dir}/bam/{sample}-sorted.stats", out_dir = out_dir, sample = samples, donor = donors),
        expand("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz.tbi", out_dir = out_dir, sample = samples, donor = donors, chroms = chroms, i = chunks),
        expand("{out_dir}/vcf/{donor}-annotated-var.vcf.gz", out_dir = out_dir, donor = donors)
        #expand("{out_dir}/vcf/{donor}-var.vcf", out_dir = out_dir, donor = matches.keys()),

def get_donor(sample, matches):
    for donor, samples in matches.items():
        if sample in samples['crypt_samples']:
            return donor
    raise KeyError(f"Sample {sample} not found in matches dictionary")

# from downloaded crams
rule align_and_sort:
    input:
        cram = lambda wc: f"{in_dir}/{get_donor(wc.sample, matches)}/{wc.sample}_final.cram"

    output:
        bam_sort = temp("{out_dir}/bam/{sample}-sortednoRG.bam")

    shell:
        """
        echo "Input CRAM file: {input.cram}"
        echo "Output BAM file: {output.bam_sort}"
        samtools sort -o {output.bam_sort} {input.cram}
        echo "Successfully sorted CRAM to BAM: {output.bam_sort}"
        """

rule addrg:
    input:
        bam_sort = rules.align_and_sort.output.bam_sort
    output:
        sort_bam_RG = "{out_dir}/bam/{sample}-sorted.bam"
    params:
        donor = lambda wildcards: get_donor(wildcards.sample, matches)
    shell:
        """
        samtools addreplacerg -r "@RG\\tID:{params.donor}_{wildcards.sample}\\tSM:{params.donor}_{wildcards.sample}\\tLB:{params.donor}_{wildcards.sample}\\tPL:Illumina" {input.bam_sort} | \
        samtools view -b > {output.sort_bam_RG}
        """

rule index_bam:
    input:
        bam_sort = rules.addrg.output.sort_bam_RG
    output:
        bai = "{out_dir}/bam/{sample}-sorted.bam.bai"
    shell:
        """
        samtools index {input.bam_sort}
        """

rule check_bam:
    input:
        bam_sort = rules.addrg.output.sort_bam_RG
    output:
        stats = "{out_dir}/bam/{sample}-sorted.stats"
    shell:
        """
        samtools stats {input.bam_sort} > {output.stats}
        """

rule generateregions:
    input:
        reference = reference,
        out_dir = out_dir
    params:
        chunks = chunks
    output:
        regions = expand("{out_dir}/regions/chunk.{chroms}.region.{i}.bed", i = chunks, out_dir = out_dir, chroms = chroms)
    shell:
        """
        python /uufs/chpc.utah.edu/common/HIPAA/u1264408/u1264408/Git/SEMIColon/scripts/variant_calling/fasta_generate_regions.py --fai {input.reference} --chunks 20  --bed {input.out_dir}/regions/chunk
        """


rule make_bam_list:
    input:
        bam_sort = lambda wildcard: expand("{out_dir}/bam/{sample}-sorted.bam", sample=samples, out_dir=out_dir),
        bai = lambda wildcard: expand("{out_dir}/bam/{sample}-sorted.bam.bai", sample=samples, out_dir=out_dir)
    params:
        location = "{out_dir}/bam/"
    output:
        bam_file_list = temp("{out_dir}/bam_list_{donor}.txt")
    shell:
        """
        python /uufs/chpc.utah.edu/common/HIPAA/u1264408/u1264408/Git/SEMIColon/scripts/LeeSixdata/create_bam_list.py {wildcards.donor} {params.location} {wildcards.out_dir}
        """

# # Current filtering:
# ### min-alternate-count 2
### qsum 40
rule freebayes_variant_calling:
    input:
        bam_file_list = "{out_dir}/bam_list_{donor}.txt",
        ref = reference,
        regions = "{out_dir}/regions/chunk.{chroms}.region.{i}.bed"
    output:
        full = temp("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf"),
    resources:
        mem_mb = 32000
    shell:
        """
        freebayes --min-alternate-count 2 --min-alternate-qsum 40 -f {input.ref} -t {input.regions} -L {input.bam_file_list} > {output.full}
        """


rule compress_chunks:
    input:
        chunk_vcf = "{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf"
    output:
        chunk_zip = temp("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz")
    shell:
        """
        bgzip -f {input.chunk_vcf}
        """

rule index_chunks:
    input:
        chunk_zip = "{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz"
    output:
        chunk_zip_index = temp("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz.tbi")
    shell:
        """
        tabix -f -p vcf {input.chunk_zip}
        """

rule ConcatVCFs:
    input:
        chunk_zip = lambda wildcard: expand("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz", i = chunks, out_dir = wildcard.out_dir, donor = wildcard.donor, chroms = chroms),
        chunk_vcf_index = lambda wildcard: expand("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz.tbi", i = chunks, out_dir = wildcard.out_dir, donor = wildcard.donor, chroms = chroms),
    output:
        vcf = "{out_dir}/vcf/{donor}-var.vcf.gz",
        vcf_index = "{out_dir}/vcf/{donor}-var.vcf.gz.tbi"
    shell:
        """
        bcftools concat -a {input.chunk_zip} -o {output.vcf}
        tabix -f -p vcf {output.vcf}
        """


rule decomposeVCFs:
    input:
        vcf = "{out_dir}/vcf/{donor}-var.vcf.gz",
        fasta = reference
    output:
        clean_vcf = temp("{out_dir}/vcf/{donor}-clean-var.vcf.gz"),
    shell:
        """
        bcftools norm -m - {input.vcf} -w 10000 -f {input.fasta} -O b -o {output.clean_vcf}
        """

rule annotateVCFs:
    input:
        clean_vcf = "{out_dir}/vcf/{donor}-clean-var.vcf.gz",
    output:
        annotated_vcf = "{out_dir}/vcf/{donor}-annotated-var.vcf.gz",
    shell:
        """
        slivar expr -g /scratch/ucgd/lustre/common/data/Slivar/db/gnomad.hg37.zip -v {input.clean_vcf} -o {output.annotated_vcf}
        """

