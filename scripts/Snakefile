configfile: '/uufs/chpc.utah.edu/common/HIPAA/u1264408/u1264408/Git/SEMIColon/data/config/snakemake_config/CCconfig.yaml'
top_dir = config["top_directory"]
in_dir: str = config["input_directory"]
out_dir: str = config["output_directory"]
reference: str = config["reference_genome"]
reference_index: str = config["reference_genome_index"]
samples: list[str] = config["samples"]
donors: list[str] = config["donors"]
nchunks = config["nchunks"]
chunks = list(range(1,nchunks+1))
chroms = ["chr1","chr2","chr3","chr4","chr6","chr7","chr8","chr9","chr10","chr11","chr12","chr13","chr14","chr15","chr16","chr17",
"chr18","chr19","chr20","chr21","chr22","chrX","chrY"]

matches = {
    'GB115': {'crypt_samples': ['Laurel-1', 'Laurel-2', "Laurel-3", "Laurel-5", "Laurel-6", "Laurel-7",
    "Laurel-8", "Laurel-10", "Laurel-11", "Laurel-12", "Laurel-13", "Laurel-14", "Laurel-15", "Laurel-16", "Laurel-17"]},
}

### IMPORTS ###
import os
import glob
import re

# Wildcard constraints; should all be alphanumeric
wildcard_constraints:
    sample = "[A-Za-z0-9_-]+",
    chroms = "[A-Za-z0-9]+",
    i = "[0-9]+",
    out_dir = out_dir,
    donor = "[A-Za-z0-9]+"

# Ensure sample_files is initialized
sample_files = {}

for sample in samples:
    matching_dirs = glob.glob(f"{top_dir}/**/{sample}", recursive=True)

    for subdir in matching_dirs:
        subdir_files = glob.glob(f"{subdir}/*.fastq.gz")

        r1_files = [f for f in subdir_files if "_R1" in f]
        r2_files = [f for f in subdir_files if "_R2" in f]

        if sample not in sample_files:
            sample_files[sample] = {'R1': [], 'R2': []}

        sample_files[sample]['R1'].extend(r1_files)
        sample_files[sample]['R2'].extend(r2_files)

def get_donor(sample, matches):
    for donor, samples in matches.items():
        if sample in samples['crypt_samples']:
            return donor
    raise KeyError(f"Sample {sample} not found in matches dictionary")

rule all:
    input:
        expand("{out_dir}/bam/{sample}-sorted.bam.bai", out_dir = out_dir, sample = samples, donor = donors),
        expand("{out_dir}/bam/{sample}-sorted.stats", out_dir = out_dir, sample = samples),
        expand("{out_dir}/bam/{sample}-sorted.stats", out_dir = out_dir, sample = samples),
        expand("{out_dir}/mosdepth/{sample}.mosdepth.global.dist.txt", out_dir = out_dir, sample = samples),
        expand("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz.tbi", out_dir = out_dir, sample = samples, donor = donors, chroms = chroms, i = chunks),
        expand("{out_dir}/vcf/{donor}-annotated-var.vcf.gz", out_dir = out_dir, donor = donors),
        expand("{in_dir}/merged/{sample}_R1.fastq.gz", sample=samples, in_dir = in_dir),
        expand("{in_dir}/merged/{sample}_R2.fastq.gz", sample=samples, in_dir = in_dir),
        expand("{out_dir}/reports/{sample}-fastp-report.json", sample=samples, out_dir=out_dir)
        #expand("{out_dir}/vcf/{donor}-var.vcf", out_dir = out_dir, donor = matches.keys())

# Snakemake rule for merging fastq.gz files
rule merge_fastq:
    input:
        R1=lambda wildcards: sample_files.get(wildcards.sample, {}).get('R1', []),
        R2=lambda wildcards: sample_files.get(wildcards.sample, {}).get('R2', [])
    output:
        R1_merged = "{in_dir}/merged/{sample}_R1.fastq.gz",
        R2_merged ="{in_dir}/merged/{sample}_R2.fastq.gz"
    shell:
        """
        zcat {input.R1} | gzip > {output.R1_merged}
        zcat {input.R2} | gzip > {output.R2_merged}
        """

# Rule 'fastp' preprocesses the reads by detecting and removing adapters,
# filtering reads with an average quality less than 20, and producing
# filtered fastq reads with preprocessing reports.
rule fastp:
    input:
        R1 = in_dir + "/merged/{sample}_R1.fastq.gz",
        R2 = in_dir + "/merged/{sample}_R2.fastq.gz"
    output:
        r1_clean = "{out_dir}/fastq/{sample}_R1.clean.fastq",
        r2_clean = "{out_dir}/fastq/{sample}_R2.clean.fastq",
        html_report = "{out_dir}/reports/{sample}-fastp-report.html",
        json_report = "{out_dir}/reports/{sample}-fastp-report.json"
    shell:
        """
        fastp --in1 {input.R1} --in2 {input.R2} \
        --out1 {output.r1_clean} --out2 {output.r2_clean} \
        --thread 16 \
        --disable_quality_filtering --disable_adapter_trimming --disable_trim_poly_g \
        --html {output.html_report} \
        --json {output.json_report}
        """

rule generate_regions:
    input:
        index = reference_index,
        out_dir = out_dir,
        chroms = chroms
    params:
        chunks = chunks
    output:
        regions = "{out_dir}/regions/chunk.{chroms}.region.{i}.bed"
    shell:
        """
        mkdir -p {input.out_dir}/regions
        python /variant_calling/fasta_generate_regions.py --fai {input.index} --chunks 9  --bed {input.out_dir}/regions/chunk
        """

# Aligns cleaned fastq reads to the defined reference genome
# Mark duplicates and extract discordant and split reads from sam files
# Convert to bam (exclude unmapped reads with -F 4)

rule align_and_sort:
    input:
        r1_clean = rules.fastp.output.r1_clean,
        r2_clean = rules.fastp.output.r2_clean,
        ref = reference
    output:
        bam_sort = temp("{out_dir}/bam/{sample}-sortednoRG.bam")
    threads: 8
    shell:
        """
        bwa mem -t {threads} {input.ref} {input.r1_clean} {input.r2_clean} | samblaster | samtools view -b | samtools sort -o {output.bam_sort}
        """

rule add_rg:
    input:
        bam_sort = rules.align_and_sort.output.bam_sort
    output:
        sort_bam_RG = "{out_dir}/bam/{sample}-sorted.bam"
    params:
        donor = lambda wildcards: get_donor(wildcards.sample, matches)
    shell:
        """
        samtools addreplacerg -r "@RG\\tID:{params.donor}_{wildcards.sample}\\tSM:{params.donor}_{wildcards.sample}\\tLB:{params.donor}_{wildcards.sample}\\tPL:Illumina" {input.bam_sort} | \
        samtools view -b > {output.sort_bam_RG}
        """

rule index_bam:
    input:
        bam_sort = rules.add_rg.output.sort_bam_RG
    output:
        bai = "{out_dir}/bam/{sample}-sorted.bam.bai"
    shell:
        """
        samtools index {input.bam_sort}
        """

rule samtools_stats:
    input:
        bam_sort = rules.add_rg.output.sort_bam_RG
    output:
        stats = "{out_dir}/bam/{sample}-sorted.stats",
    shell:
        """
        samtools stats {input.bam_sort} > {output.stats}
        """

rule mosdepth:
    input:
        bam_sort = rules.add_rg.output.sort_bam_RG
    output:
        "{out_dir}/mosdepth/{sample}.mosdepth.global.dist.txt",
    shell:
        """
        module load mosdepth
        bash quality_control/mosdepth.sh
        """

rule make_bam_list:
    input:
        bam_sort = lambda wildcard: expand("{out_dir}/bam/{sample}-sorted.bam", sample=samples, out_dir=out_dir),
        bai = lambda wildcard: expand("{out_dir}/bam/{sample}-sorted.bam.bai", sample=samples, out_dir=out_dir),
    params:
        location = "{out_dir}/bam/"
    output:
        bam_file_list = temp("{out_dir}/bam_list_{donor}.txt")
    shell:
        """
        python variant_calling/create_bam_list.py {wildcards.donor} {params.location} {wildcards.out_dir}
        """

# Current filtering:
### min-alternate-count 2
## qsum 40
rule freebayes_variant_calling:
    input:
        bam_file_list = "{out_dir}/bam_list_{donor}.txt",
        ref = reference,
        regions = "{out_dir}/regions/chunk.{chroms}.region.{i}.bed"
    output:
        full = temp("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf"),
    resources:
        mem_mb = 32000
    shell:
        """
        mkdir -p {wildcards.out_dir}/vcf/{wildcards.chroms}
        freebayes --min-alternate-count 2 --min-alternate-qsum 40 -f {input.ref} -t {input.regions} -L {input.bam_file_list} > {output.full}
        """


rule compress_chunks:
    input:
        chunk_vcf = "{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf"
    output:
        chunk_zip = temp("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz")
    shell:
        """
        bgzip -f {input.chunk_vcf}
        """

rule index_chunks:
    input:
        chunk_zip = "{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz"
    output:
        chunk_zip_index = temp("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz.tbi")
    shell:
        """
        tabix -f -p vcf {input.chunk_zip}
        """

rule concat_vcfs:
    input:
        chunk_zip = lambda wildcard: expand("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz", i = chunks, out_dir = wildcard.out_dir, donor = wildcard.donor, chroms = chroms),
        chunk_vcf_index = lambda wildcard: expand("{out_dir}/vcf/{chroms}/{donor}-variants.{i}.vcf.gz.tbi", i = chunks, out_dir = wildcard.out_dir, donor = wildcard.donor, chroms = chroms),
    output:
        vcf = "{out_dir}/vcf/{donor}-var.vcf.gz",
        vcf_index = "{out_dir}/vcf/{donor}-var.vcf.gz.tbi"
    shell:
        """
        bcftools concat -a {input.chunk_zip} -o {output.vcf}
        tabix -f -p vcf {output.vcf}
        """


rule decompose_vcfs:
    input:
        vcf = "{out_dir}/vcf/{donor}-var.vcf.gz",
        fasta = reference
    output:
        clean_vcf = temp("{out_dir}/vcf/{donor}-clean-var.vcf.gz"),
    shell:
        """
        bcftools norm -m - {input.vcf} -w 10000 -f {input.fasta} -O b -o {output.clean_vcf}
        """

rule annotate_VCFs:
    input:
        clean_vcf = "{out_dir}/vcf/{donor}-clean-var.vcf.gz",
    output:
        annotated_vcf = "{out_dir}/vcf/{donor}-annotated-var.vcf.gz",
    shell:
        """
        slivar expr -g /scratch/ucgd/lustre/common/data/Slivar/db/gnomad.hg38.genomes.v3.fix.zip -v {input.clean_vcf} -o {output.annotated_vcf}
        """

