# Snakefile: intersect matching vcf.gz files from two directories
# Use config.yaml to override defaults if desired.

import os
from pathlib import Path

# configfile: "config.yaml"   # uncomment if you use a config.yaml

DIR_A = Path(config.get("dir_a", "/uufs/chpc.utah.edu/common/HIPAA/u1264408/u1264408/Git/SEMIColon/data/output/CellCut/results_ds/nuclear_all"))
DIR_B = Path(config.get("dir_b", "/uufs/chpc.utah.edu/common/HIPAA/u1264408/u1264408/Git/SEMIColon/data/output/CellCut/results/nuclear_all"))
OUT_DIR = Path(config.get("out_dir", "/uufs/chpc.utah.edu/common/HIPAA/u1264408/u1264408/Git/SEMIColon/data/output/CellCut/overlap_variants"))

BCFTOOLS = config.get("bcftools", "bcftools")
BGZIP = config.get("bgzip", "bgzip")
TABIX = config.get("tabix", "tabix")

def read_count_table(path):
    """
    Read CHROM POS REF ALT VAF-like files into dict keyed by (chrom,pos,ref,alt).
    Skip blank lines, comment lines (starting '#'), and header lines
    (e.g. 'CHROM POS REF ALT VAF' or any line whose first column == 'CHROM').
    """
    d = {}
    if not path:
        return d
    if not os.path.exists(path):
        return d

    with open(path) as fh:
        for raw in fh:
            line = raw.strip()
            if not line:
                continue
            # skip explicit comment headers
            if line.startswith("#"):
                continue

            parts = line.split()
            if not parts:
                continue

            # Detect and skip a header row even if it's not commented:
            # e.g. "CHROM POS REF ALT VAF" (case-insensitive),
            # or any header containing the string "VAF" in the first 6 cols.
            first = parts[0].upper()
            upper_parts = [p.upper() for p in parts[:6]]  # check only first few cols
            if first == "CHROM" or "VAF" in upper_parts or "POSITION" in upper_parts:
                # this is a header line â€” skip it
                continue

            # require at least 5 columns: CHROM POS REF ALT VAF
            if len(parts) < 5:
                continue

            chrom, pos, ref, alt, vaf = parts[0], parts[1], parts[2], parts[3], parts[4]
            key = (chrom, pos, ref, alt)
            d[key] = vaf

    return d


CONDA_ISEC = config.get("conda_isec", None)

# --- discover .vcf.gz files robustly using pathlib ---
def list_vcf_basenames(dirpath: Path):
    """Return set of basenames like 'sample.vcf.gz' present in dirpath."""
    if not dirpath.exists():
        return set()
    return {p.name for p in dirpath.glob("*.vcf.gz")}

baseA = list_vcf_basenames(DIR_A)
baseB = list_vcf_basenames(DIR_B)

common_basenames = sorted(baseA & baseB)

# turn 'sample.vcf.gz' -> 'sample' even if sample contains dots
def strip_vcf_gz(basename: str) -> str:
    assert basename.endswith(".vcf.gz")
    return basename[:-7]   # remove the trailing ".vcf.gz"

SAMPLES = [strip_vcf_gz(b) for b in common_basenames]

# If you want to debug what samples were found, uncomment the print below
# print("SAMPLES:", SAMPLES)

def a_vcf(sample): return str(DIR_A / f"{sample}.vcf.gz")
def b_vcf(sample): return str(DIR_B / f"{sample}.vcf.gz")
def out_vcf(sample): return str(OUT_DIR / f"{sample}.intersection.vcf.gz")
def out_tbi(sample): return str(OUT_DIR / f"{sample}.intersection.vcf.gz.tbi")

rule all:
    input:
        # intersections (vcf + tbi)
        expand(os.path.join(str(OUT_DIR), "{sample}.intersection.vcf.gz"), sample=SAMPLES),
        expand(os.path.join(str(OUT_DIR), "{sample}.intersection.vcf.gz.tbi"), sample=SAMPLES),

        # merged per-sample tables created by merge_indels and merge_snvs
        expand(os.path.join(str(OUT_DIR), "{sample}.indels_merged.tsv"), sample=SAMPLES),
        expand(os.path.join(str(OUT_DIR), "{sample}.snvs_merged.tsv"), sample=SAMPLES),


rule isec_pair:
    input:
        a = lambda wc: a_vcf(wc.sample),
        b = lambda wc: b_vcf(wc.sample)
    output:
        vcf = os.path.join(str(OUT_DIR), "{sample}.intersection.vcf.gz"),
        tbi = os.path.join(str(OUT_DIR), "{sample}.intersection.vcf.gz.tbi")
    threads: 1
    # Option 1: use CONDA_ISEC from config (string path to env YAML), else comment/uncomment below:
    conda: CONDA_ISEC
    # Option 2 (uncomment to use fixed env file next to Snakefile):
    # conda: "envs/isec.yaml"
    params:
        tmpdir = lambda wc: os.path.join("/tmp", f"snk_isec_{wc.sample}")
    shell:
        """
        module load bcftools
        mkdir -p "{params.tmpdir}"
        mkdir -p "$(dirname {output.vcf})"

        tabix -f -p vcf {input.a}
        tabix -f -p vcf {input.b}
        bcftools isec -p "{params.tmpdir}" {input.a} {input.b}

        shared_vcf="{params.tmpdir}/0002.vcf"

        if [[ -s "$shared_vcf" ]]; then
            echo "[OK] shared variants found for {wildcards.sample} -> compressing"
            {BGZIP} -c "$shared_vcf" > "{output.vcf}"
        else
            echo "[INFO] no shared variants for {wildcards.sample} -> creating header-only VCF"
            {BCFTOOLS} view -h "{input.a}" | {BGZIP} -c > "{output.vcf}"
        fi

        tabix -f -p vcf "{output.vcf}"
        rm -rf "{params.tmpdir}"
        """

# --- improved finder helpers: return None if not found ---
def find_indel_file(dirpath: Path, sample: str):
    """Return str(path) of first match or None."""
    if not dirpath.exists():
        return None
    matches = list(dirpath.rglob(f"{sample}*.indels_count.txt"))
    return str(matches[0]) if matches else None

def find_snv_file(dirpath: Path, sample: str):
    """Return str(path) of first match or None."""
    if not dirpath.exists():
        return None
    matches = list(dirpath.rglob(f"{sample}*.snv_count.txt"))
    return str(matches[0]) if matches else None

def a_indels(sample): return find_indel_file(DIR_A, sample)
def b_indels(sample): return find_indel_file(DIR_B, sample)
def a_snvs(sample): return find_snv_file(DIR_A, sample)
def b_snvs(sample): return find_snv_file(DIR_B, sample)

# --- merge_indels: input returns [] when not found (Snakemake-acceptable) ---
rule merge_indels:
    input:
        a = lambda wc: [a_indels(wc.sample)] if a_indels(wc.sample) else [],
        b = lambda wc: [b_indels(wc.sample)] if b_indels(wc.sample) else []
    output:
        merged = os.path.join(str(OUT_DIR), "{sample}.indels_merged.tsv")
    threads: 1
    params:
        sample = lambda wc: wc.sample
    run:
        import csv, os

        out_path = output.merged
        os.makedirs(os.path.dirname(out_path), exist_ok=True)

        # input.a and input.b are lists (maybe empty)
        a_path = input.a[0] if input.a else None
        b_path = input.b[0] if input.b else None

        def read_indel_table(path):
            d = {}
            if not path:
                return d
            if not os.path.exists(path):
                return d
            with open(path) as fh:
                for raw in fh:
                    line = raw.strip()
                    if not line or line.startswith("#"):
                        continue
                    parts = line.split()
                    if len(parts) < 5:
                        continue
                    chrom, pos, ref, alt, vaf = parts[0], parts[1], parts[2], parts[3], parts[4]
                    d[(chrom, pos, ref, alt)] = vaf
            return d

        a_table = read_count_table(a_path)
        b_table = read_count_table(b_path)


        keys = sorted(set(a_table.keys()) & set(b_table.keys()),
              key=lambda k: (k[0],
                             int(k[1]) if k[1].isdigit() else k[1],
                             k[2], k[3]))


        with open(out_path, "w") as out:
            writer = csv.writer(out, delimiter="\t", lineterminator="\n")
            writer.writerow(["CHROM","POS","REF","ALT","VAF_DS","VAF_FB"])
            for k in keys:
                vds = a_table.get(k, "NA")
                vfb = b_table.get(k, "NA")
                writer.writerow([k[0], k[1], k[2], k[3], vds, vfb])

        print(f"[merge_indels] wrote {out_path} (rows: {len(keys)})")

# --- merge_snvs: same approach as merge_indels ---
rule merge_snvs:
    input:
        a = lambda wc: [a_snvs(wc.sample)] if a_snvs(wc.sample) else [],
        b = lambda wc: [b_snvs(wc.sample)] if b_snvs(wc.sample) else []
    output:
        merged = os.path.join(str(OUT_DIR), "{sample}.snvs_merged.tsv")
    threads: 1
    params:
        sample = lambda wc: wc.sample
    run:
        import csv, os

        out_path = output.merged
        os.makedirs(os.path.dirname(out_path), exist_ok=True)

        a_path = input.a[0] if input.a else None
        b_path = input.b[0] if input.b else None

        def read_snv_table(path):
            d = {}
            if not path:
                return d
            if not os.path.exists(path):
                return d
            with open(path) as fh:
                for raw in fh:
                    line = raw.strip()
                    if not line or line.startswith("#"):
                        continue
                    parts = line.split()
                    if len(parts) < 5:
                        continue
                    chrom, pos, ref, alt, vaf = parts[0], parts[1], parts[2], parts[3], parts[4]
                    d[(chrom, pos, ref, alt)] = vaf
            return d

        a_table = read_count_table(a_path)
        b_table = read_count_table(b_path)


        def sort_key(k):
            chrom, pos = k[0], k[1]
            try:
                posnum = int(pos)
            except Exception:
                posnum = pos
            return (chrom, posnum, k[2], k[3])

        keys = sorted(set(a_table.keys()) & set(b_table.keys()),
              key=lambda k: (k[0],
                             int(k[1]) if k[1].isdigit() else k[1],
                             k[2], k[3]))


        with open(out_path, "w") as out:
            writer = csv.writer(out, delimiter="\t", lineterminator="\n")
            writer.writerow(["CHROM","POS","REF","ALT","VAF_DS","VAF_FB"])
            for k in keys:
                vds = a_table.get(k, "NA")
                vfb = b_table.get(k, "NA")
                writer.writerow([k[0], k[1], k[2], k[3], vds, vfb])

        print(f"[merge_snvs] wrote {out_path} (rows: {len(keys)})")
